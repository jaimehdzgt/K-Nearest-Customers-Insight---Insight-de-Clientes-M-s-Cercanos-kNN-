{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Lookalike Finder ‚Äì Insight de Clientes M√°s Cercanos (kNN)\n",
    "\n",
    "**Empresa:** `NovaRetail Group`  \n",
    "**Objetivo:** construir un motor de clientes \"gemelos\" (lookalike modeling) para apoyar estrategias de **marketing dirigido y CRM**.\n",
    "\n",
    "Este notebook muestra, paso a paso:\n",
    "\n",
    "- Carga y exploraci√≥n del historial de clientes.\n",
    "- Limpieza de columnas con formatos sucios y mixtos.\n",
    "- Construcci√≥n de un vector de **features por cliente** (demogr√°ficas y de comportamiento).\n",
    "- Normalizaci√≥n de variables y entrenamiento de un modelo **k-Nearest Neighbors (kNN)**.\n",
    "- B√∫squeda de clientes **lookalike** para clientes VIP.\n",
    "- Visualizaciones (distribuciones y PCA 2D).\n",
    "- Conclusiones din√°micas basadas en los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° **Nota t√©cnica**  \n",
    "> Este notebook est√° dise√±ado para ejecutarse dentro de **GitHub Codespaces** o cualquier entorno con Python 3, con el archivo:\n",
    ">\n",
    "> `data/customer_lookalike_raw_100k.xlsx`\n",
    ">\n",
    "> que contiene el hist√≥rico de clientes de NovaRetail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalaci√≥n de dependencias\n",
    "\n",
    "Ejecuta esta celda solo la primera vez en un entorno nuevo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scikit-learn matplotlib openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n inicial e importaci√≥n de librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Ruta al archivo de datos (aj√∫stala si cambias la estructura del repo)\n",
    "DATA_PATH = os.path.join(\"data\", \"customer_lookalike_raw_100k.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y exploraci√≥n inicial del dataset\n",
    "\n",
    "En esta secci√≥n cargamos el historial de clientes y revisamos:\n",
    "\n",
    "- Dimensiones del dataset.\n",
    "- Primeras filas.\n",
    "- Estad√≠sticos b√°sicos de variables num√©ricas.\n",
    "- Distribuci√≥n de clientes VIP vs no VIP.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontr√≥ el archivo de datos en {DATA_PATH}. \"\n",
    "        \"Aseg√∫rate de que exista `customer_lookalike_raw_100k.xlsx` en la carpeta `data/`.\"\n",
    "    )\n",
    "\n",
    "df_raw = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print(\"Shape del dataset (filas, columnas):\", df_raw.shape)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Descripci√≥n estad√≠stica de algunas variables clave\n",
    "cols_desc = [\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"tenure_months\",\n",
    "    \"total_orders\",\n",
    "    \"days_since_last_purchase\",\n",
    "    \"pct_reordered\",\n",
    "    \"avg_items_per_order\",\n",
    "    \"vip_score\",\n",
    "]\n",
    "\n",
    "display(df_raw[cols_desc].describe().T)\n",
    "\n",
    "print(\"\\nDistribuci√≥n de vip_flag (proporci√≥n):\")\n",
    "display(df_raw[\"vip_flag\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClientes por regi√≥n (cruda):\")\n",
    "display(df_raw[\"region\"].value_counts().head())\n",
    "\n",
    "print(\"\\nClientes por categor√≠a favorita:\")\n",
    "display(df_raw[\"fav_category\"].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza de datos y tratamiento de columnas sucias\n",
    "\n",
    "El dataset contiene varias columnas con formatos mixtos y valores sucios:\n",
    "\n",
    "- Edades con sufijos de texto (`\"35 years\"`, `\"N/A\"`, etc.).\n",
    "- Montos con s√≠mbolos de moneda y comas.\n",
    "- Porcentajes como strings (`\"45.2%\"`).\n",
    "- Regiones con espacios, min√∫sculas o typos (`\" nortee \"`).\n",
    "- Valores especiales en recencia (`\"never\"`, `9999`, etc.).\n",
    "\n",
    "Definimos funciones auxiliares para normalizar cada uno de estos casos.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_age_from_dirty(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte age_str_dirty a num√©rico cuando sea necesario.\"\"\"\n",
    "    def _parse(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        x = str(x).strip()\n",
    "        if x in [\"\", \"N/A\", \"na\", \"NA\", \"None\"]:\n",
    "            return np.nan\n",
    "        x = x.replace(\"years\", \"\").strip()\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return col.apply(_parse)\n",
    "\n",
    "\n",
    "def parse_total_spent(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte total_spent_dirty a float (quita s√≠mbolos de moneda y comas).\"\"\"\n",
    "    def _parse(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        x = str(x).strip()\n",
    "        if x == \"\":\n",
    "            return np.nan\n",
    "        x = x.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return col.apply(_parse)\n",
    "\n",
    "\n",
    "def parse_pct(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte porcentajes tipo '45.2%' o 45.2 a proporci√≥n [0,1].\"\"\"\n",
    "    def _parse(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if isinstance(x, (int, float)):\n",
    "            return float(x)\n",
    "        x = str(x).strip()\n",
    "        if x.endswith(\"%\"):\n",
    "            try:\n",
    "                return float(x[:-1]) / 100.0\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        try:\n",
    "            val = float(x)\n",
    "            if val > 1:\n",
    "                val = val / 100.0\n",
    "            return val\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return col.apply(_parse)\n",
    "\n",
    "\n",
    "def clean_region(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Limpia regiones con espacios, min√∫sculas o typos ('Nortee' ‚Üí 'Norte').\"\"\"\n",
    "    def _clean(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        x = str(x).strip()\n",
    "        if x.lower() == \"nortee\":\n",
    "            return \"Norte\"\n",
    "        return x.title()\n",
    "    return col.apply(_clean)\n",
    "\n",
    "\n",
    "def parse_days_since_last_purchase(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convierte d√≠as desde √∫ltima compra, manejando 'never' y valores extremos.\"\"\"\n",
    "    def _parse(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if isinstance(x, (int, float)):\n",
    "            val = int(x)\n",
    "            if val > 999:\n",
    "                val = 999\n",
    "            return val\n",
    "        x = str(x).strip().lower()\n",
    "        if x == \"never\":\n",
    "            return 999\n",
    "        try:\n",
    "            val = int(float(x))\n",
    "            if val > 999:\n",
    "                val = 999\n",
    "            return val\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return col.apply(_parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construcci√≥n de features por cliente\n",
    "\n",
    "En esta secci√≥n:\n",
    "\n",
    "- Creamos columnas limpias (`*_clean`).\n",
    "- Imputamos valores faltantes con la mediana (num√©ricas) o `'Unknown'` (categ√≥ricas).\n",
    "- Generamos un vector de features num√©ricas + categ√≥ricas (one-hot encoding).\n",
    "- Escalamos las features con `StandardScaler` para aplicar kNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_feature_matrix(df: pd.DataFrame):\n",
    "    \"\"\"Limpia y transforma el DataFrame en una matriz de features escaladas.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Columnas limpias\n",
    "    df[\"age_clean\"] = df[\"age\"]\n",
    "    mask_age_missing = df[\"age_clean\"].isna()\n",
    "    if \"age_str_dirty\" in df.columns:\n",
    "        df.loc[mask_age_missing, \"age_clean\"] = parse_age_from_dirty(\n",
    "            df.loc[mask_age_missing, \"age_str_dirty\"]\n",
    "        )\n",
    "\n",
    "    df[\"total_spent\"] = parse_total_spent(df[\"total_spent_dirty\"])\n",
    "    df[\"pct_reordered_clean\"] = parse_pct(df[\"pct_reordered_dirty\"])\n",
    "    df[\"region_clean\"] = clean_region(df[\"region_dirty\"])\n",
    "    df[\"days_since_last_purchase_clean\"] = parse_days_since_last_purchase(\n",
    "        df[\"days_since_last_purchase_dirty\"]\n",
    "    )\n",
    "\n",
    "    # Imputaci√≥n simple de NA\n",
    "    numeric_cols_to_fill = [\n",
    "        \"age_clean\",\n",
    "        \"total_spent\",\n",
    "        \"pct_reordered_clean\",\n",
    "        \"days_since_last_purchase_clean\",\n",
    "        \"income\",\n",
    "        \"tenure_months\",\n",
    "        \"total_orders\",\n",
    "        \"avg_items_per_order\",\n",
    "    ]\n",
    "    for col in numeric_cols_to_fill:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    cat_cols_to_fill = [\"gender\", \"region_clean\", \"fav_category\", \"signup_channel\"]\n",
    "    for col in cat_cols_to_fill:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "    # Definici√≥n de features\n",
    "    num_features = [\n",
    "        \"age_clean\",\n",
    "        \"income\",\n",
    "        \"tenure_months\",\n",
    "        \"total_orders\",\n",
    "        \"days_since_last_purchase_clean\",\n",
    "        \"pct_reordered_clean\",\n",
    "        \"avg_items_per_order\",\n",
    "        \"total_spent\",\n",
    "    ]\n",
    "    cat_features = [\"region_clean\", \"fav_category\", \"signup_channel\"]\n",
    "\n",
    "    X_num = df[num_features].copy()\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "    X_cat = ohe.fit_transform(df[cat_features])\n",
    "    ohe_feature_names = ohe.get_feature_names_out(cat_features)\n",
    "\n",
    "    X = np.hstack([X_num.values, X_cat])\n",
    "    feature_names = num_features + list(ohe_feature_names)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return df, X_scaled, feature_names, scaler, ohe\n",
    "\n",
    "\n",
    "df_clean, X_scaled, feature_names, scaler, ohe = build_feature_matrix(df_raw)\n",
    "\n",
    "print(\"Shape de la matriz de features escaladas:\", X_scaled.shape)\n",
    "print(\"N√∫mero de features:\", len(feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del modelo k-Nearest Neighbors (kNN)\n",
    "\n",
    "Entrenamos un modelo de **NearestNeighbors** sobre las features escaladas.\n",
    "Este modelo ser√° el motor que nos permite buscar clientes similares (lookalikes).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_knn(X_scaled: np.ndarray,\n",
    "              n_neighbors: int = 11) -> NearestNeighbors:\n",
    "    \"\"\"Entrena un modelo kNN sobre la matriz de features escaladas.\"\"\"\n",
    "    knn = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=\"euclidean\",\n",
    "        algorithm=\"auto\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    knn.fit(X_scaled)\n",
    "    return knn\n",
    "\n",
    "\n",
    "def find_lookalikes(customer_id: str,\n",
    "                    k: int,\n",
    "                    df: pd.DataFrame,\n",
    "                    X_scaled: np.ndarray,\n",
    "                    knn: NearestNeighbors) -> pd.DataFrame:\n",
    "    \"\"\"Devuelve los k clientes m√°s similares a `customer_id` y sus distancias.\"\"\"\n",
    "    if customer_id not in df[\"customer_id\"].values:\n",
    "        raise ValueError(f\"customer_id {customer_id} no encontrado en el DataFrame.\")\n",
    "\n",
    "    idx = df.index[df[\"customer_id\"] == customer_id][0]\n",
    "\n",
    "    distances, indices = knn.kneighbors(\n",
    "        X_scaled[idx].reshape(1, -1),\n",
    "        n_neighbors=k + 1  # incluye al propio cliente\n",
    "    )\n",
    "\n",
    "    neighbor_indices = indices[0]\n",
    "    neighbor_distances = distances[0]\n",
    "\n",
    "    # Quitamos al propio cliente\n",
    "    mask_not_self = neighbor_indices != idx\n",
    "    neighbor_indices = neighbor_indices[mask_not_self][:k]\n",
    "    neighbor_distances = neighbor_distances[mask_not_self][:k]\n",
    "\n",
    "    result = df.iloc[neighbor_indices].copy()\n",
    "    cols = [\n",
    "        \"customer_id\",\n",
    "        \"vip_flag\",\n",
    "        \"income\",\n",
    "        \"total_orders\",\n",
    "        \"region_clean\",\n",
    "        \"fav_category\",\n",
    "        \"total_spent\",\n",
    "    ]\n",
    "    existing_cols = [c for c in cols if c in result.columns]\n",
    "    result = result[existing_cols]\n",
    "    result[\"distance\"] = neighbor_distances\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "knn = train_knn(X_scaled, n_neighbors=11)\n",
    "print(\"Modelo kNN entrenado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejemplo de uso: clientes lookalike para un cliente VIP\n",
    "\n",
    "A continuaci√≥n:\n",
    "\n",
    "- Seleccionamos aleatoriamente un cliente con `vip_flag = 1`.\n",
    "- Calculamos sus **10 vecinos m√°s cercanos** en el espacio de features.\n",
    "- Inspeccionamos la tabla resultante para interpretar el resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filtramos clientes VIP\n",
    "vip_customers = df_clean[df_clean[\"vip_flag\"] == 1]\n",
    "num_vip = len(vip_customers)\n",
    "num_total = len(df_clean)\n",
    "\n",
    "print(f\"N√∫mero de clientes VIP: {num_vip} ({num_vip / num_total:.2%} del total)\")\n",
    "\n",
    "# Tomamos un VIP de ejemplo\n",
    "example_vip = vip_customers.sample(1, random_state=42)\n",
    "vip_id = example_vip[\"customer_id\"].iloc[0]\n",
    "\n",
    "print(f\"Cliente VIP de ejemplo: {vip_id}\")\n",
    "display(example_vip)\n",
    "\n",
    "# Buscamos sus lookalikes\n",
    "lookalikes_vip = find_lookalikes(\n",
    "    customer_id=vip_id,\n",
    "    k=10,\n",
    "    df=df_clean,\n",
    "    X_scaled=X_scaled,\n",
    "    knn=knn,\n",
    ")\n",
    "\n",
    "display(lookalikes_vip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizaciones\n",
    "\n",
    "En esta secci√≥n generamos visualizaciones para entender mejor:\n",
    "\n",
    "- C√≥mo se distribuyen las variables clave de comportamiento.\n",
    "- C√≥mo se posicionan los clientes (y en particular los VIP) en un espacio 2D reducido con PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Distribuci√≥n de variables clave\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "important_features = [\n",
    "    \"income\",\n",
    "    \"total_orders\",\n",
    "    \"total_spent\",\n",
    "    \"days_since_last_purchase_clean\",\n",
    "]\n",
    "\n",
    "for col in important_features:\n",
    "    if col not in df_clean.columns:\n",
    "        continue\n",
    "    plt.figure()\n",
    "    df_clean[col].hist(bins=50)\n",
    "    plt.title(f\"Distribuci√≥n de {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Proyecci√≥n PCA 2D (VIP vs No VIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    \"pc1\": X_pca[:, 0],\n",
    "    \"pc2\": X_pca[:, 1],\n",
    "    \"vip_flag\": df_clean[\"vip_flag\"].values,\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mask_vip = df_pca[\"vip_flag\"] == 1\n",
    "\n",
    "plt.scatter(\n",
    "    df_pca.loc[~mask_vip, \"pc1\"],\n",
    "    df_pca.loc[~mask_vip, \"pc2\"],\n",
    "    alpha=0.3,\n",
    "    s=5,\n",
    "    label=\"No VIP\",\n",
    ")\n",
    "plt.scatter(\n",
    "    df_pca.loc[mask_vip, \"pc1\"],\n",
    "    df_pca.loc[mask_vip, \"pc2\"],\n",
    "    alpha=0.6,\n",
    "    s=8,\n",
    "    label=\"VIP\",\n",
    ")\n",
    "\n",
    "plt.title(\"Proyecci√≥n PCA de clientes (VIP vs No VIP)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculamos algunos indicadores para alimentar las conclusiones\n",
    "vip_ratio = num_vip / num_total\n",
    "\n",
    "mean_income_vip = df_clean.loc[df_clean[\"vip_flag\"] == 1, \"income\"].mean()\n",
    "mean_income_nonvip = df_clean.loc[df_clean[\"vip_flag\"] == 0, \"income\"].mean()\n",
    "\n",
    "mean_spent_vip = df_clean.loc[df_clean[\"vip_flag\"] == 1, \"total_spent\"].mean()\n",
    "mean_spent_nonvip = df_clean.loc[df_clean[\"vip_flag\"] == 0, \"total_spent\"].mean()\n",
    "\n",
    "mean_orders_vip = df_clean.loc[df_clean[\"vip_flag\"] == 1, \"total_orders\"].mean()\n",
    "mean_orders_nonvip = df_clean.loc[df_clean[\"vip_flag\"] == 0, \"total_orders\"].mean()\n",
    "\n",
    "conclusions_md = \"\"\"\\\n",
    "### Resumen anal√≠tico\n",
    "\n",
    "- La base de clientes utilizada contiene **{num_total:,} clientes**, de los cuales\n",
    "  **{num_vip:,}** est√°n marcados como VIP (**{vip_ratio:.2%}** del total).\n",
    "- Los clientes VIP presentan, en promedio, un ingreso aproximado de\n",
    "  **${mean_income_vip:,.0f}**, frente a **${mean_income_nonvip:,.0f}** en el resto de clientes.\n",
    "- En t√©rminos de gasto acumulado:\n",
    "  - VIP: **${mean_spent_vip:,.0f}** en promedio.\n",
    "  - No VIP: **${mean_spent_nonvip:,.0f}** en promedio.\n",
    "- Los VIP tambi√©n muestran una mayor actividad:\n",
    "  - √ìrdenes promedio VIP: **{mean_orders_vip:,.1f}**.\n",
    "  - √ìrdenes promedio no VIP: **{mean_orders_nonvip:,.1f}**.\n",
    "\n",
    "### Interpretaci√≥n de negocio\n",
    "\n",
    "- El modelo kNN permite encontrar, para un cliente VIP concreto (`{vip_id}`),\n",
    "  un conjunto de clientes con **perfiles muy similares** (sus lookalikes), lo que abre la puerta a:\n",
    "    - Extender campa√±as de **retenci√≥n o lealtad** hacia clientes que a√∫n no est√°n en el programa,\n",
    "    - Dise√±ar campa√±as de **cross-sell / upsell** sobre grupos con alto potencial de valor.\n",
    "- La proyecci√≥n PCA muestra que los clientes VIP tienden a concentrarse en ciertas regiones del espacio\n",
    "  de caracter√≠sticas, lo que respalda la l√≥gica de buscar vecinos en dicho espacio en lugar de usar\n",
    "  reglas simples por edad o ticket promedio.\n",
    "- Integrar este motor en los flujos de CRM permitir√≠a:\n",
    "    - Priorizar a qui√©n impactar primero con recursos limitados,\n",
    "    - Medir el **lift de conversi√≥n** al comparar campa√±as con y sin lookalikes,\n",
    "    - Reducir el costo por conversi√≥n al enfocar los esfuerzos en perfiles de alto potencial.\n",
    "\"\"\".format(\n",
    "    num_total=num_total,\n",
    "    num_vip=num_vip,\n",
    "    vip_ratio=vip_ratio,\n",
    "    mean_income_vip=mean_income_vip,\n",
    "    mean_income_nonvip=mean_income_nonvip,\n",
    "    mean_spent_vip=mean_spent_vip,\n",
    "    mean_spent_nonvip=mean_spent_nonvip,\n",
    "    mean_orders_vip=mean_orders_vip,\n",
    "    mean_orders_nonvip=mean_orders_nonvip,\n",
    "    vip_id=vip_id,\n",
    ")\n",
    "\n",
    "display(Markdown(conclusions_md))\n"
   ]
  }
 ]
}

